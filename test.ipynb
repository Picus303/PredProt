{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import build_transformer\n",
    "\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer(dropout=0.1,\n",
    "\t\t\t\t\t\tsource_vocab_size=8000+1, target_vocab_size=292, context_length=900 - 3 + 1,\n",
    "\t\t\t\t\t\tencoder_block_count=6,\n",
    "\t\t\t\t\t\tencoder_self_attention_head_count=8,\n",
    "\t\t\t\t\t\tencoder_self_attention_abstraction_coef=0.15,\n",
    "\t\t\t\t\t\tencoder_feed_forward_abstraction_coef=4,\n",
    "\t\t\t\t\t\tdim=256, epsilon=1e-9)\n",
    "\n",
    "checkpoint = torch.load(\"weights/tr_model_10\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load('X.pt', weights_only=True).int().to(\"cuda\")\n",
    "L = torch.load('L.pt', weights_only=True).int().to(\"cuda\")\n",
    "Y = torch.load('Y.pt', weights_only=True).half().to(\"cuda\")\n",
    "\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(X) * train_ratio)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "X_train, X_test = random_split(X, [train_size, test_size], generator)\n",
    "L_train, L_test = random_split(L, [train_size, test_size], generator)\n",
    "Y_train, Y_test = random_split(Y, [train_size, test_size], generator)\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(L_train), len(L_test))\n",
    "print(len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = torch.load('T.pt', weights_only=True).int().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "dataset = CustomDataset(X_test, Y_test)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_loop(model, data, thresholds, num_classes=292):\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()  # BCE avec logits, donc pas besoin de Sigmoid séparé\n",
    "    loss_data = {\"num_loss\": 0, \"sum\": 0}\n",
    "\n",
    "    confusion_matrix = torch.zeros((num_classes, 2, 2), dtype=torch.int32)  # Matrice de confusion par classe\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batch_iterator = tqdm(data, desc=f\"Processing batches\")\n",
    "\n",
    "        for x, y in batch_iterator:\n",
    "            pred = model(x, mask=None)\n",
    "\n",
    "            # Calcul de la perte\n",
    "            loss = loss_fn(pred, y)\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Calcul de la perte totale\n",
    "            loss_data[\"num_loss\"] += 1\n",
    "            loss_data[\"sum\"] += loss.item()\n",
    "\n",
    "            # Appliquer une sigmoïde pour obtenir les probabilités\n",
    "            pred_probs = torch.sigmoid(pred)\n",
    "\n",
    "            # Calcul des prédictions\n",
    "            pred_labels = (pred_probs > thresholds).int()\n",
    "\n",
    "            # Calcul des statistiques\n",
    "            for i in range(num_classes):\n",
    "                tp = ((pred_labels[:, i] == 1) & (y[:, i] == 1)).sum()\n",
    "                tn = ((pred_labels[:, i] == 0) & (y[:, i] == 0)).sum()\n",
    "                fp = ((pred_labels[:, i] == 1) & (y[:, i] == 0)).sum()\n",
    "                fn = ((pred_labels[:, i] == 0) & (y[:, i] == 1)).sum()\n",
    "\n",
    "                confusion_matrix[i, 0, 0] += tn\n",
    "                confusion_matrix[i, 0, 1] += fp\n",
    "                confusion_matrix[i, 1, 0] += fn\n",
    "                confusion_matrix[i, 1, 1] += tp\n",
    "    \n",
    "    print(f\"Loss: {loss_data['sum'] / loss_data['num_loss']}\")\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = testing_loop(model, loader, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_stats(confusion_matrix_total, num_classes=292):\n",
    "\t# Initialisation des listes pour stocker les pourcentages\n",
    "\tdetection_accuracy = []\n",
    "\tidentification_accuracy = []\n",
    "\toverall_accuracy = []\n",
    "\n",
    "\t# Parcourir chaque classe pour extraire les métriques\n",
    "\tfor i in range(num_classes):\n",
    "\t\tTN = confusion_matrix_total[i, 0, 0].item()\n",
    "\t\tFP = confusion_matrix_total[i, 0, 1].item()\n",
    "\t\tFN = confusion_matrix_total[i, 1, 0].item()\n",
    "\t\tTP = confusion_matrix_total[i, 1, 1].item()\n",
    "\n",
    "\t\t# Pourcentage d'identifications réussies (VP / (VP + FN))\n",
    "\t\tif (TP + FN) > 0:\n",
    "\t\t\tdetection_accuracy.append(TP / (TP + FN))\n",
    "\t\telse:\n",
    "\t\t\tdetection_accuracy.append(0.0)\n",
    "\n",
    "\t\t# Pourcentage de bonnes identifications (VP / (VP + FP))\n",
    "\t\tif (TP + FP) > 0:\n",
    "\t\t\tidentification_accuracy.append(TP / (TP + FP))\n",
    "\t\telse:\n",
    "\t\t\tidentification_accuracy.append(0.0)\n",
    "\n",
    "\t\t# Pourcentage de bonne décision globale ((VP + VN) / (VP + VN + FP + FN))\n",
    "\t\ttotal = TP + TN + FP + FN\n",
    "\t\tif total > 0:\n",
    "\t\t\toverall_accuracy.append((TP + TN) / total)\n",
    "\t\telse:\n",
    "\t\t\toverall_accuracy.append(0.0)\n",
    "\n",
    "\t# Convertir les listes en tableaux numpy pour un traitement facile\n",
    "\tdetection_accuracy = np.array(detection_accuracy)\n",
    "\tidentification_accuracy = np.array(identification_accuracy)\n",
    "\toverall_accuracy = np.array(overall_accuracy)\n",
    "\n",
    "\t# Affichage des statistiques\n",
    "\tprint(f\"Pourcentage moyen d'identifications réussies: {detection_accuracy.mean() * 100:.2f}%\")\n",
    "\tprint(f\"Pourcentage moyen de bonnes identifications: {identification_accuracy.mean() * 100:.2f}%\")\n",
    "\tprint(f\"Pourcentage moyen de bonne décision globale: {overall_accuracy.mean() * 100:.2f}%\")\n",
    "\n",
    "\tprint(f\"Pourcentage médian d'identifications réussies: {np.median(detection_accuracy) * 100:.2f}%\")\n",
    "\tprint(f\"Pourcentage médian de bonnes identifications: {np.median(identification_accuracy) * 100:.2f}%\")\n",
    "\tprint(f\"Pourcentage médian de bonne décision globale: {np.median(overall_accuracy) * 100:.2f}%\")\n",
    "\n",
    "\tprint(f\"Pire classe pour le pourcentage d'identifications réussies: {detection_accuracy.min() * 100:.2f}%\")\n",
    "\tprint(f\"Pire classe pour le pourcentage de bonnes identifications: {identification_accuracy.min() * 100:.2f}%\")\n",
    "\tprint(f\"Pire classe pour le pourcentage de bonne décision globale: {overall_accuracy.min() * 100:.2f}%\")\n",
    "\n",
    "\tprint(f\"Meilleure classe pour le pourcentage d'identifications réussies: {detection_accuracy.max() * 100:.2f}%\")\n",
    "\tprint(f\"Meilleure classe pour le pourcentage de bonnes identifications: {identification_accuracy.max() * 100:.2f}%\")\n",
    "\tprint(f\"Meilleure classe pour le pourcentage de bonne décision globale: {overall_accuracy.max() * 100:.2f}%\")\n",
    "\n",
    "\t# Création des barplots\n",
    "\tclasses = np.arange(num_classes)\n",
    "\n",
    "\t# Plot 1: Pourcentage d'identifications réussies\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.bar(classes, detection_accuracy, color='b')\n",
    "\tplt.title('Pourcentage d\\'identifications réussies (VP / (VP + FN))')\n",
    "\tplt.xlabel('Classes')\n",
    "\tplt.ylabel('Pourcentage')\n",
    "\tplt.ylim(0, 1)  # Les pourcentages sont entre 0 et 1\n",
    "\tplt.show()\n",
    "\n",
    "\t# Plot 2: Pourcentage de bonnes identifications\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.bar(classes, identification_accuracy, color='g')\n",
    "\tplt.title('Pourcentage de bonnes identifications (VP / (VP + FP))')\n",
    "\tplt.xlabel('Classes')\n",
    "\tplt.ylabel('Pourcentage')\n",
    "\tplt.ylim(0, 1)\n",
    "\tplt.show()\n",
    "\n",
    "\t# Plot 3: Pourcentage de bonne décision globale\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.bar(classes, overall_accuracy, color='r')\n",
    "\tplt.title('Pourcentage de bonne décision ((VP + VN) / (VP + VN + FP + FN))')\n",
    "\tplt.xlabel('Classes')\n",
    "\tplt.ylabel('Pourcentage')\n",
    "\tplt.ylim(0, 1)\n",
    "\tplt.show()\n",
    "    \n",
    "plot_confusion_stats(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
