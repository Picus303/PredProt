{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer(dropout=0.1,\n",
    "\t\t\t\t\t\tsource_vocab_size=8000+1, target_vocab_size=292, context_length=900 - 3 + 1,\n",
    "\t\t\t\t\t\tencoder_block_count=6,\n",
    "\t\t\t\t\t\tencoder_self_attention_head_count=8,\n",
    "\t\t\t\t\t\tencoder_self_attention_abstraction_coef=0.15,\n",
    "\t\t\t\t\t\tencoder_feed_forward_abstraction_coef=4,\n",
    "\t\t\t\t\t\tdim=256, epsilon=1e-9)\n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Nombre total de param√®tres apprenables : {total_params}\")\n",
    "\n",
    "model_path = None\n",
    "if model_path:\n",
    "\tmodel.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load('X.pt', weights_only=True).int().to(\"cuda\")\n",
    "L = torch.load('L.pt', weights_only=True).int().to(\"cuda\")\n",
    "Y = torch.load('Y.pt', weights_only=True).half().to(\"cuda\")\n",
    "\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(X) * train_ratio)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "X_train, X_test = random_split(X, [train_size, test_size], generator)\n",
    "L_train, L_test = random_split(L, [train_size, test_size], generator)\n",
    "Y_train, Y_test = random_split(Y, [train_size, test_size], generator)\n",
    "\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(L_train), len(L_test))\n",
    "print(len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\t\"batch_size\": 16,\n",
    "\t\"epochs\": 100,\n",
    "\t\"lr\": 1e-4,\n",
    "\t\"epsilon\": 1e-9,\n",
    "\t\"weigths_folder\": \"weights/\",\n",
    "\t\"weights_file\": \"tr_model_\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, L, Y):\n",
    "        self.X = X\n",
    "        self.L = L\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.L[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "dataset = CustomDataset(X_train, L_train, Y_train)\n",
    "loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "masks = torch.ones((5, 5, 5), device=\"cuda\")\n",
    "for i in range(5-1):\n",
    "\tmasks[i, :, i+1:] = 0\n",
    "\n",
    "print(masks[torch.tensor([0, 2, 4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, data, config, info):\n",
    "\n",
    "\tif not os.path.exists(config[\"weigths_folder\"]):\n",
    "\t\tos.makedirs(config[\"weigths_folder\"])\n",
    "\t\n",
    "\toptimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], eps=config[\"epsilon\"], weight_decay=1e-5)\n",
    "\tloss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\tmasks = torch.ones((info[\"context_length\"], 1, info[\"context_length\"], info[\"context_length\"]), device=\"cuda\")\n",
    "\tfor i in range(info[\"context_length\"]-1):\n",
    "\t\tmasks[i, :, :, i+1:] = 0\n",
    "\n",
    "\tfor epoch in range(config[\"epochs\"]):\n",
    "\t\tmodel.train()\n",
    "\t\tbatch_iterator = tqdm(data, desc=f\"Prcessing epoch {epoch:02d}\")\n",
    "\t\tfor x, l, y in batch_iterator:\n",
    "\t\t\tm = masks[l - 1]\n",
    "\t\t\tpred = model(x, mask=m)\n",
    "\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tbatch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\n",
    "\t\tmodel_path = config[\"weigths_folder\"] + config[\"weights_file\"] + str(epoch)\n",
    "\t\ttorch.save({\"state\": model.state_dict()}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {\n",
    "\t\"context_length\": 900 - 3 + 1,\n",
    "}\n",
    "\n",
    "training_loop(model, loader, config, info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
